{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import pdb\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "\n",
    "import model\n",
    "from anchors import Anchors\n",
    "import losses\n",
    "from dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import coco_eval\n",
    "import csv_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_config = {}\n",
    "notebook_config['dataset']='coco'                #'Dataset type, must be one of csv or coco.'\n",
    "notebook_config['coco_path']='../coco/'                #'Path to COCO directory'\n",
    "notebook_config['csv_train']=None                #'Path to file containing training annotations (see readme)'\n",
    "notebook_config['csv_classes']=None                #'Path to file containing class list (see readme)'\n",
    "notebook_config['csv_val']=None                #'Path to file containing validation annotations (optional, see readme)'\n",
    "notebook_config['depth']=18                #'Resnet depth, must be one of 18, 34, 50, 101, 152'\n",
    "notebook_config['epochs']=100                #'Number of epochs'\n",
    "notebook_config['use_gpu'] = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_config['dataset'] == 'coco':\n",
    "\n",
    "    if notebook_config['coco_path'] is None:\n",
    "        raise ValueError('Must provide --coco_path when training on COCO,')\n",
    "\n",
    "    dataset_train = CocoDataset(notebook_config['coco_path'], set_name='train2017', transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "    dataset_val = CocoDataset(notebook_config['coco_path'], set_name='val2017', transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "elif notebook_config['dataset'] == 'csv':\n",
    "\n",
    "    if notebook_config['csv_train'] is None:\n",
    "        raise ValueError('Must provide --csv_train when training on COCO,')\n",
    "\n",
    "    if notebook_config['csv_classes'] is None:\n",
    "        raise ValueError('Must provide --csv_classes when training on COCO,')\n",
    "\n",
    "\n",
    "    dataset_train = CSVDataset(train_file=notebook_config['csv_train'], class_list=notebook_config['csv_classes'], transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "\n",
    "    if notebook_config['csv_val'] is None:\n",
    "        dataset_val = None\n",
    "        print('No validation annotations provided.')\n",
    "    else:\n",
    "        dataset_val = CSVDataset(train_file=notebook_config['csv_val'], class_list=notebook_config['csv_classes'], transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "else:\n",
    "    raise ValueError('Dataset type not understood (must be csv or coco), exiting.')\n",
    "\n",
    "sampler = AspectRatioBasedSampler(dataset_train, batch_size=16, drop_last=False)\n",
    "dataloader_train = DataLoader(dataset_train, num_workers=8, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "if dataset_val is not None:\n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if notebook_config['depth'] == 18:\n",
    "    retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif notebook_config['depth'] == 34:\n",
    "    retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif notebook_config['depth'] == 50:\n",
    "    retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif notebook_config['depth'] == 101:\n",
    "    retinanet = model.resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "elif notebook_config['depth'] == 152:\n",
    "    retinanet = model.resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "else:\n",
    "    raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')        \n",
    "\n",
    "\n",
    "\n",
    "if notebook_config['use_gpu']:\n",
    "    retinanet = retinanet.cuda()\n",
    "    retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "\n",
    "retinanet.training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "retinanet.train()\n",
    "retinanet.module.freeze_bn()\n",
    "print('Num training images: {}'.format(len(dataset_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_num in range(notebook_config['epochs']):\n",
    "\n",
    "    retinanet.train()\n",
    "    retinanet.module.freeze_bn()\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for iter_num, data in enumerate(dataloader_train):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "\n",
    "            loss = classification_loss + regression_loss\n",
    "\n",
    "            if bool(loss == 0):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_hist.append(float(loss))\n",
    "\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "            print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "            del classification_loss\n",
    "            del regression_loss\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    if notebook_config['dataset'] == 'coco':\n",
    "\n",
    "        print('Evaluating dataset')\n",
    "\n",
    "        coco_eval.evaluate_coco(dataset_val, retinanet)\n",
    "\n",
    "    elif notebook_config['dataset'] == 'csv' and notebook_config['csv_val'] is not None:\n",
    "\n",
    "        print('Evaluating dataset')\n",
    "\n",
    "        mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
    "\n",
    "\n",
    "    scheduler.step(np.mean(epoch_loss))    \n",
    "\n",
    "    torch.save(retinanet.module, '{}_retinanet_{}.pt'.format(notebook_config['dataset'], epoch_num))\n",
    "\n",
    "retinanet.eval()\n",
    "\n",
    "torch.save(retinanet, 'model_final.pt'.format(epoch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
