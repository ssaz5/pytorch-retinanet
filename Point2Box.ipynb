{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import torch\n",
    "\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from point2box.src.myutils import get_scenes_from_dir, make_dataset_from_scenes, MOTDataset_BBwise, row_wise_iou\n",
    "\n",
    "from point2box.src.myutils import MOTDataset_Imagewise\n",
    "\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "import joblib\n",
    "\n",
    "import ipywidgets as wdg  # Using the ipython notebook widgets\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import pdb\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision\n",
    "\n",
    "import model\n",
    "from anchors import Anchors\n",
    "import losses\n",
    "from dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import coco_eval\n",
    "import csv_eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_config = {}\n",
    "notebook_config['dataset']='mot'                #'Dataset type, must be one of csv or coco.'\n",
    "notebook_config['coco_path']='../coco/'                #'Path to COCO directory'\n",
    "notebook_config['mot_path']='../Datasets/'\n",
    "notebook_config['point_dist_path']='../point2box/point_dist'\n",
    "notebook_config['csv_train']=None                #'Path to file containing training annotations (see readme)'\n",
    "notebook_config['csv_classes']=None                #'Path to file containing class list (see readme)'\n",
    "notebook_config['csv_val']=None                #'Path to file containing validation annotations (optional, see readme)'\n",
    "notebook_config['depth']=18                #'Resnet depth, must be one of 18, 34, 50, 101, 152'\n",
    "notebook_config['epochs']=20                #'Number of epochs'\n",
    "notebook_config['use_gpu'] = True\n",
    "notebook_config['batch_size'] = 1\n",
    "notebook_config['load_sampler'] = True\n",
    "notebook_config['save_sampler'] = not notebook_config['load_sampler']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_scenes = get_scenes_from_dir(notebook_config['mot_path'])\n",
    "# split_point = int(0.9*len(all_scenes))\n",
    "# print(len(all_scenes), split_point)\n",
    "# train_scenes = all_scenes[:split_point]\n",
    "# val_scenes = all_scenes[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val_scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = MOTDataset_Imagewise(train_scenes, path_for_points= notebook_config['point_dist_path'], \n",
    "#                                        transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "\n",
    "# dataset_val = MOTDataset_Imagewise(val_scenes, path_for_points= notebook_config['point_dist_path'], \n",
    "#                                      transform=transforms.Compose([Normalizer(), Resizer()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if notebook_config['load_sampler']:\n",
    "#     sampler =  joblib.load('sampler.pkl')\n",
    "#     dataloader_train = joblib.load('data_loader.pkl')\n",
    "#     if dataset_val is not None:\n",
    "#         sampler_val =  joblib.load('sampler_val.pkl')\n",
    "#         dataloader_val = joblib.load('dataloader_val.pkl')\n",
    "# else:\n",
    "#     sampler = AspectRatioBasedSampler(dataset_train, batch_size=notebook_config['batch_size'], drop_last=False)\n",
    "#     dataloader_train = DataLoader(dataset_train, num_workers=8, collate_fn=collater, batch_sampler=sampler)\n",
    "#     if dataset_val is not None:\n",
    "#         sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "#         dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if notebook_config['save_sampler']:\n",
    "#     joblib.dump(sampler, 'sampler.pkl')\n",
    "#     joblib.dump(dataloader_train, 'data_loader.pkl')\n",
    "#     if dataset_val is not None:\n",
    "#         joblib.dump(sampler_val, 'sampler_val.pkl')\n",
    "#         joblib.dump(dataloader_val, 'dataloader_val.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if notebook_config['depth'] == 18:\n",
    "#     retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# elif notebook_config['depth'] == 34:\n",
    "#     retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# elif notebook_config['depth'] == 50:\n",
    "#     retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# elif notebook_config['depth'] == 101:\n",
    "#     retinanet = model.resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# elif notebook_config['depth'] == 152:\n",
    "#     retinanet = model.resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "# else:\n",
    "#     raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')        \n",
    "\n",
    "\n",
    "\n",
    "# if notebook_config['use_gpu']:\n",
    "#     retinanet = retinanet.cuda()\n",
    "#     retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "\n",
    "# retinanet.training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "# loss_hist = collections.deque(maxlen=500)\n",
    "\n",
    "# retinanet.train()\n",
    "# retinanet.module.freeze_bn()\n",
    "# print('Num training images: {}'.format(len(dataset_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(dataloader_train):\n",
    "#     print(i)\n",
    "#     print(data['img'].shape)\n",
    "#     if i>5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retinanet.train()\n",
    "# retinanet.module.freeze_bn()\n",
    "\n",
    "# epoch_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch_num in range(notebook_config['epochs']):\n",
    "\n",
    "#     retinanet.train()\n",
    "#     retinanet.module.freeze_bn()\n",
    "\n",
    "#     epoch_loss = []\n",
    "\n",
    "#     for iter_num, data in enumerate(dataloader_train):\n",
    "#         try:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "\n",
    "#             classification_loss = classification_loss.mean()\n",
    "#             regression_loss = regression_loss.mean()\n",
    "\n",
    "#             loss = classification_loss + regression_loss\n",
    "\n",
    "#             if bool(loss == 0):\n",
    "#                 continue\n",
    "\n",
    "#             loss.backward()\n",
    "\n",
    "#             torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_hist.append(float(loss))\n",
    "\n",
    "#             epoch_loss.append(float(loss))\n",
    "\n",
    "#             print('Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "#             del classification_loss\n",
    "#             del regression_loss\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "\n",
    "#     if notebook_config['dataset'] == 'coco':\n",
    "\n",
    "#         print('Evaluating dataset')\n",
    "\n",
    "#         coco_eval.evaluate_coco(dataset_val, retinanet)\n",
    "\n",
    "#     elif notebook_config['dataset'] == 'csv' and notebook_config['csv_val'] is not None:\n",
    "\n",
    "#         print('Evaluating dataset')\n",
    "\n",
    "#         mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
    "\n",
    "\n",
    "#     scheduler.step(np.mean(epoch_loss))    \n",
    "\n",
    "#     torch.save(retinanet.module, '{}_retinanet_{}.pt'.format(notebook_config['dataset'], epoch_num))\n",
    "\n",
    "# retinanet.eval()\n",
    "\n",
    "# torch.save(retinanet, 'model_final.pt'.format(epoch_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retinanet = torch.load('mot_retinanet_0.pt')\n",
    "\n",
    "# retinanet.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retinanet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_val.length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retinanet.eval()\n",
    "# mAP = csv_eval.evaluate(dataset_val, retinanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(dataset_val, 'dataset_val.pkl')\n",
    "dataset_val = joblib.load('dataset_val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val.get_point = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 1080)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-be6dffd3e486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/point2box/src/myutils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mpts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;31m#         print(pt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "sample = dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sample['points'].astype(np.int32)\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample['img'].shape[:2])\n",
    "point_channel = np.zeros(sample['img'].shape[:2])\n",
    "point_channel[points[:, -1::]] = 255\n",
    "# plt.imshow(sample['img'].numpy()/np.max(sample['img'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
